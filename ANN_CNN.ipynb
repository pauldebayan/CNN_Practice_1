{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import onnx\n",
    "import onnxscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randint(1, 100, (100,)) #min, max, size\n",
    "x2 = torch.randint(1, 100, (100,))\n",
    "\n",
    "y = (2*x2+5*x1) # we need to find this formula\n",
    "#y = [(torch.square(y))*111 for y in y] \n",
    "\n",
    "data = {'Feature 1': x1, 'Feature 2': x2, 'Y': y}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loc[#Rows, #Columns], loc is label-based\n",
    "#iloc[:, index], iloc is integer-based\n",
    "\n",
    "#print(df.loc[0]) # Gives the entire row\n",
    "\n",
    "#Fetch row as well as columns\n",
    "# print(df.loc[0:5, \"Feature 2\"])\n",
    "\n",
    "print(f\" Feature 1: {df.iloc[0, 0]}\")\n",
    "print(f\" Feature 2: {df.iloc[0, 1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validate, Test split\n",
    "\n",
    "train_data = df.iloc[0:60, :]\n",
    "# print(train_data)\n",
    "validate_data = df.iloc[60:80, :]\n",
    "test_data = df.iloc[80:100, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized weights and biases\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change hyperparameters only for Validation dataset\n",
    "\n",
    "learning_rate = 1e-3\n",
    "#batch_size = 2\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Problem Statement</th>\n",
    "        <th>Hidden Layer</th>\n",
    "        <th>Output Layer</th>\n",
    "        <th>Loss Function</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Regression</td>\n",
    "        <td>ReLU</td>\n",
    "        <td>Linear</td>\n",
    "        <td>MSE/MAE/Kuber</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Binary Classification</td>\n",
    "        <td>ReLU</td>\n",
    "        <td>Sigmoid</td>\n",
    "        <td>Binary Cross Entropy</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Multiclass Classification</td>\n",
    "        <td>ReLU</td>\n",
    "        <td>Softmax</td>\n",
    "        <td>Categorical/Sparse Cross Entropy</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    size = len(train_data)\n",
    "    #print(size)\n",
    "\n",
    "    accumulate_loss = 0\n",
    "    for i in range(size): \n",
    "        \n",
    "        feature1 = torch.tensor(train_data.iloc[i, 0]).float()\n",
    "        feature2 = torch.tensor(train_data.iloc[i, 1]).float()\n",
    "        y = torch.tensor(train_data.iloc[i, 2]).float()\n",
    "\n",
    "        X = torch.stack([feature1, feature2], dim=0) #nn.Sequential... nn.Linear(2, 10),\n",
    "\n",
    "        pred = model(X)\n",
    "        # print(type(pred))\n",
    "        # print(type(y))\n",
    "        loss = loss_fn(pred, y)\n",
    "    \n",
    "        # Backpropagation\n",
    "        loss.backward() # Find Gradients\n",
    "        optimizer.step() # Update weights\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "        accumulate_loss += loss.item()\n",
    "\n",
    "    return accumulate_loss/size\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_loop():\n",
    "    size = len(validate_data)\n",
    "\n",
    "    accumulate_loss = 0\n",
    "    for i in range(size): \n",
    "        \n",
    "        feature1 = torch.tensor(validate_data.iloc[i, 0]).float()\n",
    "        feature2 = torch.tensor(validate_data.iloc[i, 1]).float()\n",
    "        y = torch.tensor(validate_data.iloc[i, 2]).float()\n",
    "\n",
    "        X = torch.stack([feature1, feature2], dim=0)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "    \n",
    "        accumulate_loss += loss.item()\n",
    "\n",
    "    return accumulate_loss/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x+1 for x in range(epochs)]\n",
    "\n",
    "trainLossArr = []\n",
    "validateLossArr = []\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_loop()\n",
    "    trainLossArr.append(train_loss)\n",
    "\n",
    "    # Validate\n",
    "    validate_loss = validate_loop()\n",
    "    validateLossArr.append(validate_loss)\n",
    "\n",
    "    print(f\"Epoch {i+1} - Training Loss: {train_loss}, Validation Loss: {validate_loss}\")\n",
    " \n",
    "# print(x)\n",
    "# print(trainLossArr)\n",
    "plt.plot(x, trainLossArr, 'b', label=\"Train\")\n",
    "plt.plot(x, validateLossArr, 'g', label=\"Valid\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')\n",
    "model.load_state_dict(torch.load('model.pt', weights_only=True))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(test_data)\n",
    "\n",
    "correct = 0\n",
    "for i in range(size): \n",
    "    \n",
    "    feature1 = torch.tensor(test_data.iloc[i, 0]).float()\n",
    "    feature2 = torch.tensor(test_data.iloc[i, 1]).float()\n",
    "    y = torch.tensor(test_data.iloc[i, 2]).float()\n",
    "\n",
    "    X = torch.stack([feature1, feature2], dim=0)\n",
    "\n",
    "    pred = model(X)\n",
    "\n",
    "    if pred.squeeze().int() == y:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct/size*100\n",
    "\n",
    "print(f\"Accracy: {accuracy}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Testing\n",
    "\n",
    "x1 = torch.tensor(10, dtype=torch.float)\n",
    "x2 = torch.tensor(20, dtype=torch.float)\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "\n",
    "X = torch.stack([x1, x2], dim=0)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "pred = model(X)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.youtube.com/watch?v=l8_fZPHasdo\">How we see color - RGB</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGB Image Channels - 3<br/>\n",
    "Grayscale Image Channels - 1\n",
    "\n",
    "CNN Working:\n",
    "\n",
    "![CNN](images/CNN.jpg)\n",
    "\n",
    "![Image_Filter_FeatureMaps_ConvolutionLayer](images/Convolution_Layer.jpg)\n",
    "\n",
    "![Flattening](images/Flattening.jpg)\n",
    "![Flattening](images/Flattening2NN.jpg)\n",
    "\n",
    "![Filter_Applied_to_Iamge](images/FilterApplied2Image.jpg)\n",
    "![Visualizing1](images/Visualizing1.jpg)\n",
    "![Visualizing2](images/Visualizing2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since a pixel can have values from 0 to 255 \n",
    "# We create a 50x50 image and draw a Circle and a Straight line\n",
    "\n",
    "gen_img = torch.ones([50, 50])*255\n",
    "\n",
    "# Straight Line\n",
    "for i in range(50):\n",
    "    for j in range(50):\n",
    "        if i == j:\n",
    "            gen_img[i][j] = 0\n",
    "\n",
    "# Circle\n",
    "radius = torch.tensor(15)\n",
    "center = 25\n",
    "\n",
    "for i in range(90):\n",
    "    # x = r * cos(θ)\n",
    "    # y = r * sin(θ)\n",
    "\n",
    "    tensor_i = torch.tensor(i)\n",
    "    x = radius * torch.cos(tensor_i)\n",
    "    y = radius * torch.sin(tensor_i)\n",
    "\n",
    "    x = x.int()\n",
    "    y = y.int()\n",
    "\n",
    "    gen_img[x+radius][y-radius] = 0\n",
    "\n",
    "print(gen_img)\n",
    "print(gen_img.shape)\n",
    "plt.imshow(gen_img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Since it is a image it can also be downloaded\n",
    "torchvision.utils.save_image(gen_img, './images/gen_img.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YCbCr and RGB\n",
    "<a href=\"https://www.youtube.com/watch?v=3dET-EoIMM8\">YCbCr and RGB<a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting R G B individually from a image \n",
    "rgb_img = read_image('./images/RGB.jpg')\n",
    "\n",
    "#red, green, blue = rgb_img # Can also be accessed with index no. - rgb_img[1]\n",
    "#axs[i].imshow(red)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(6, 4))\n",
    "\n",
    "rgb_list = ['Red', 'Green', 'Blue']\n",
    "\n",
    "for i in range(len(rgb_list)):\n",
    "    axs[i].imshow(rgb_img[i]) # Getting the 3 channels separately\n",
    "    axs[i].set_title(rgb_list[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = read_image('./images/turtle.jpg')\n",
    "\n",
    "print(f\"Before grayscle: {img.shape}\") # [3, 50, 50], Here 3 is channel\n",
    "print(img.type) # Its a Tensor type\n",
    "\n",
    "# RGB to Grayscale\n",
    "img = torchvision.transforms.functional.rgb_to_grayscale(img, 1)\n",
    "print(f\"After grayscle: {img.shape}\") # [3, 50, 50] -> [1, 50, 50]\n",
    "\n",
    "img = img.squeeze() # Removes channel - [1, W, H] -> [W, H], all input of size 1 removed\n",
    "\n",
    "print(f\"Image shape after squeezing: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img)\n",
    "#torchvision.utils.save_image(img, 'fp.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN from PyTorch Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset from PyTorch Documentation\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.ToTensor(), \n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), \n",
    "    # transforms.Grayscale()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomImageDataset(img_dir = 'img_dir', \n",
    "                             annotations_file = 'labels.csv', \n",
    "                             transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.imshow(img) cannot display image if it is [3, H, W] we use img.permute(1, 2, 0) to make it [H, W, 3] for displaying<br/>\n",
    "<a href=\"https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.imshow.html\">(M, N, 3)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].permute(1, 2, 0)\n",
    "label = train_labels[0]\n",
    "\n",
    "print(img.shape)\n",
    "#print(img)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #afterFlattenInputNeurons = 900\n",
    "\n",
    "        # Image Shape - torch.Size([50, 50, 3])\n",
    "        # Convolution Layer 1 (sees 50x50x3 image tensor)\n",
    "        self.conv1 = nn.Conv2d(3, 100, 3, padding = 1) # 3 is Channel, 25 is output filtered image, 3 is filter size\n",
    "        \n",
    "        #After adding Maxpooling - dimensionality of the image will decrease\n",
    "        # Formula: output_size = (input_size - filter_size + 1) / stride\n",
    "        \n",
    "        # Convolution Layer 2 (sees 25x25x25 image tensor), Here the last 25 is that mentioned in 2nd parameter of conv1\n",
    "        self.conv2 = nn.Conv2d(100, 50, 3, padding = 1)\n",
    "        # Convolution Layer 3 (sees 12x12x50 image tensor)\n",
    "        self.conv3 = nn.Conv2d(50, 25, 3, padding = 1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2) #(filter/kernel size, stride)\n",
    "\n",
    "        self.activ = nn.ReLU()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(900, 100), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool(self.activ(self.conv1(x)))\n",
    "        x = self.pool(self.activ(self.conv2(x)))\n",
    "        x = self.pool(self.activ(self.conv3(x)))\n",
    "        \n",
    "        # Comment below code to get x output shape from convolution\n",
    "        # Flattening\n",
    "        x = x.view(-1, 900)\n",
    "        # Avoid overfitting\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY TO GET LINEAR LAYER INPUT\n",
    "\n",
    "dataIter = iter(train_dataloader)\n",
    "# Get first item\n",
    "img, label = next(dataIter)\n",
    "\n",
    "# print(img.shape)\n",
    "\n",
    "#afterFlattenInputNeurons\n",
    "print(\"Dimension to be put in the first Linear layer\", model.forward(img.float()).shape)\n",
    "#Dimension to be put in the first Linear layer torch.Size([1, 20, 6, 6]) - 1x20x6x6 = 720\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change hyperparameters only for Validation dataset\n",
    "\n",
    "learning_rate = 1e-3\n",
    "#batch_size = 2\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# PyTorch's Cross Entrpy Function applies a Softmax function to the output layer\n",
    "# So we do not mention Softmax activation function to the output layer \n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    size = len(train_dataloader)\n",
    "    accumulate_loss = 0\n",
    "    \n",
    "    for img, label in train_dataloader: \n",
    "        # print(img)\n",
    "        # print(label)\n",
    "\n",
    "        pred = model(img.float())\n",
    "        loss = loss_fn(pred, label)\n",
    "    \n",
    "        # Backpropagation\n",
    "        loss.backward() # Find Gradients\n",
    "        optimizer.step() # Update weights\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        accumulate_loss += loss.item()\n",
    "        #train_loss = loss.item()\n",
    "\n",
    "        \n",
    "    return accumulate_loss/size\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loop():\n",
    "    size = len(valid_dataloader)\n",
    "    accumulate_loss = 0\n",
    "    \n",
    "    for img, label in valid_dataloader: \n",
    "\n",
    "        pred = model(img.float())\n",
    "        loss = loss_fn(pred, label)\n",
    "\n",
    "        accumulate_loss += loss.item()\n",
    "        #train_loss = loss.item()\n",
    "\n",
    "        \n",
    "    return accumulate_loss/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x+1 for x in range(epochs)]\n",
    "\n",
    "trainLossArr = []\n",
    "validLossArr = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_loop()\n",
    "    trainLossArr.append(train_loss)\n",
    "\n",
    "    # Valid\n",
    "    valid_loss = train_loop()\n",
    "    validLossArr.append(valid_loss)\n",
    "\n",
    "    print(f\"Epoch {i+1} - Training Loss: {train_loss}, Validation Loss: {valid_loss}\")\n",
    " \n",
    "\n",
    "plt.plot(x, trainLossArr, 'b', label=\"Train\")\n",
    "plt.plot(x, validLossArr, 'g', label=\"Valid\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for img, label in test_dataloader:\n",
    "\n",
    "    pred = model(img.float())\n",
    "\n",
    "    if torch.argmax(pred) == label.squeeze():\n",
    "        correct += 1\n",
    "accuracy = correct/len(test_dataloader)*100\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html\">PyTorch to ONNX</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "model.load_state_dict(torch.load('model.pt', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataIter = iter(train_dataloader)\n",
    "# Get first item\n",
    "img, label = next(dataIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onnx_program = torch.onnx.export(model, img.float(), \"deploy/dolphin_or_shark.onnx\", opset_version=9)\n",
    "#onnx_program.save(\"deploy/dolphin_or_shark.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.jsdelivr.com/package/npm/onnxjs\">ONNXJS</a><br/>\n",
    "<a href=\"https://github.com/microsoft/onnxjs/blob/master/docs/operators.md\">Supported ONNX Operators</a><br/>\n",
    "<a href=\"https://learn.microsoft.com/en-us/windows/ai/windows-ml/tutorials/pytorch-convert-model\">torch.onnx.export parameters</a>\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=Vs730jsRgO8\">PyTorch ONNX Js Youtube</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://cdn.jsdelivr.net/npm/onnxjs@0.1.8/dist/onnx.min.js -P deploy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
